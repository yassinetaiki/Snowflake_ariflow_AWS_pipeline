#installation Putyy :
PuTTY est un client SSH open-source permettant d'établir des connexions à distance 
sécurisées à des serveurs Linux/Unix, Windows, ou à tout autre équipement prenant en charge
 les protocoles SSH, Telnet et Rlogin. Il est utilisé pour se connecter à des serveurs distants 
pour y exécuter des commandes ou transférer des fichiers en toute sécurité. 

#installation WinSCP
WinSCP est un client SFTP, FTP, WebDAV, S3 et SCP open source pour Windows. Il permet de 
transférer des fichiers de manière sécurisée entre un ordinateur local et un serveur distant. 
WinSCP offre une interface graphique conviviale pour la gestion des fichiers et des dossiers 
sur le serveur distant, ainsi qu'une fonctionnalité de synchronisation automatique pour faciliter 
les mises à jour de fichiers

#### data preprocessing ###
Cette partie du code a pour but de récupérer une partie du contenu des articles.

La variable partial_content est initialement définie comme une chaîne de caractères vide "". 
Elle va ensuite être remplie avec le contenu de l'article si celui-ci est présent dans le 
dictionnaire et différent de None, c'est-à-dire que l'article a été correctement récupéré et 
qu'il contient du texte.

Ensuite, si la longueur de partial_content est supérieure ou égale à 200 caractères, on garde 
les 199 premiers caractères pour éviter que la colonne "content" de la base de données ne soit 
trop longue.

Enfin, si la chaîne de caractères partial_content contient au moins un point ('.'), on définit 
trimmed_part comme étant la sous-chaîne de partial_content qui se termine juste avant le dernier
 point rencontré. Autrement, si la chaîne de caractères partial_content ne contient pas de point, 
on définit trimmed_part comme étant égale à partial_content.

Cette étape de traitement permet de nettoyer les données récupérées en ne gardant que les parties
 pertinentes de l'article et en supprimant les éventuels résidus ou caractères non pertinents.


### partie 2 code preprocessing ###
cette partie    filename = str(uuid.uuid4())
    output_file = "/home/ubuntu/{}.parquet".format(filename)
    df1 = df.drop_duplicates()
    df1.to_parquet(output_file)
    return output_file

Cette partie du code sert à sauvegarder les données nettoyées dans un fichier Parquet.

La variable filename est créée en générant un UUID (identifiant universellement unique)
 aléatoire à l'aide de la fonction uuid.uuid4(). Cela garantit que le nom du fichier sera 
unique et évite les conflits de noms de fichiers si la fonction est appelée plusieurs fois.

La variable output_file est créée en utilisant la variable filename pour créer un chemin de 
fichier complet pour le fichier Parquet qui sera créé. Le chemin de fichier complet est une
 chaîne de caractères qui spécifie le dossier et le nom du fichier.

La méthode drop_duplicates() est ensuite appelée sur l'objet DataFrame df pour supprimer toutes 
les lignes en double et créer un nouvel objet DataFrame df1.

Enfin, la méthode to_parquet() est appelée sur df1 pour écrire les données nettoyées dans un 
fichier Parquet. Le chemin de fichier complet est spécifié en utilisant la variable output_file.

La fonction retourne ensuite le chemin de fichier complet output_file pour permettre à 
l'utilisateur de récupérer facilement le fichier Parquet créé.

### Format parquet definition ###
Parquet est un format de fichier binaire et colonne destiné à stocker des données tabulaires. 
Il est conçu pour être efficace en termes de stockage et de traitement de données massives.

Contrairement à d'autres formats de fichiers comme CSV, où les données sont stockées sous forme
 de lignes de texte, les données dans un fichier Parquet sont stockées sous forme de colonnes 
sérialisées, ce qui les rend plus faciles à compresser et à traiter en parallèle.

En utilisant le format Parquet, il est possible de stocker des données massives de manière plus 
efficace en termes d'espace disque et de temps de lecture, ce qui est particulièrement important 
pour le traitement de données à grande échelle. Le format Parquet est également utilisé dans des
 systèmes de stockage distribués tels que Apache Hadoop, où il peut être utilisé pour stocker et 
traiter des données massives.

#### data pipline dag ###
# partie code 1 #
move_file_to_s3 = BashOperator(
	task_id="move_file_to_s3",
	bash_command='aws s3 mv {{ ti.xcom_pull("extract_news_info")}}  s3://irisseta',

Cette ligne de code est une commande Bash qui utilise l'outil en ligne de commande AWS CLI 
(Command Line Interface) pour déplacer un fichier stocké localement vers un bucket S3 d'AWS.

ti.xcom_pull("extract_news_info") récupère la valeur retournée par la tâche précédente 
extract_news_info, qui est le nom et le chemin du fichier enregistré localement après 
l'extraction des informations de nouvelles.
ui, il existe d'autres façons d'accéder aux valeurs de XComs dans Airflow.

Par exemple, vous pouvez utiliser ti.xcom_push() pour pousser une valeur dans un XCom à partir 
d'une tâche. Vous pouvez également utiliser le système de variables Airflow pour stocker des 
valeurs de configuration que les tâches peuvent lire et écrire.

En outre, vous pouvez utiliser des opérateurs spécialisés pour effectuer des actions spécifiques 
qui impliquent des XComs. Par exemple, le BranchPythonOperator peut être utilisé pour exécuter 
différentes tâches en fonction de la valeur d'un XCom, et le TriggerDagRunOperator peut être
 utilisé pour déclencher un autre DAG et passer des données à partir de XComs.

## partie code 2 ##
snowflake_create_table=SnowflakeOperator(
		task_id="snowflake_create_table",
		sql="""create  table if not exists helloparquet using template(select ARRAY_AGG(OBJECT_CONSTRUCT(*)) from TABLE(INFER_SCHEMA (LOCATION=>'@ramu.PUBLIC.snow_simple',FILE_FORMAT=>'parquet_format')))
        """ ,
		snowflake_conn_id="snowflake_conn"

le code SQL fourni en paramètre à l'opérateur Snowflake crée une table nommée helloparquet s'il
 n'en existe pas déjà une, en utilisant une requête qui utilise la fonction ARRAY_AGG pour agréger
 toutes les colonnes de la table spécifiée en entrée. La fonction OBJECT_CONSTRUCT est utilisée 
pour transformer chaque ligne de la table en un objet JSON. La clause USING TEMPLATE est utilisée 
pour spécifier le schéma de la table créée en fonction de la structure de la table d'entrée.

La clause using template est utilisée pour créer une table à partir d'un modèle existant. 
Dans ce cas, la table est créée en utilisant le modèle de la requête SQL suivante : 
select ARRAY_AGG(OBJECT_CONSTRUCT(*)) from TABLE(INFER_SCHEMA (LOCATION=>'@ramu.PUBLIC.snow_simple',
FILE_FORMAT=>'parquet_format')).

Cette requête utilise la fonction INFER_SCHEMA pour déterminer la structure de la table à partir
 d'un fichier Parquet stocké dans le compte Snowflake. La fonction OBJECT_CONSTRUCT est utilisée 
pour créer un objet structuré à partir de chaque ligne de la table, et la fonction ARRAY_AGG est 
utilisée pour regrouper ces objets en un tableau.

Enfin, la clause LOCATION spécifie l'emplacement du fichier Parquet à utiliser pour inférer le 
schéma de la table, et la clause FILE_FORMAT spécifie le format de fichier à utiliser.
Le paramètre snowflake_conn_id spécifie l'identifiant de la connexion Snowflake définie dans 
Airflow, qui sera utilisée pour exécuter la requête SQL dans Snowflake.

## partie code 3 ##

snowflake_copy=SnowflakeOperator(
		task_id="snowflake_copy",
		sql="""copy into ramu.PUBLIC.helloparquet from @ramu.PUBLIC.snow_simple MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE FILE_FORMAT=parquet_format
        """ ,
		snowflake_conn_id="snowflake_conn"
C'est une tâche qui utilise l'opérateur SnowflakeOperator pour copier les données d'un fichier 
Parquet stocké dans un emplacement de stockage cloud vers une table Snowflake.

Plus précisément, la tâche exécute une instruction COPY INTO qui charge les données du fichier 
Parquet stocké dans l'emplacement cloud @ramu.PUBLIC.snow_simple dans une table appelée 
helloparquet dans la base de données ramu.PUBLIC dans Snowflake. 
La clause MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE permet de correspondre les colonnes du fichier 
Parquet à celles de la table Snowflake sans tenir compte de la casse. 
La clause FILE_FORMAT=parquet_format indique que les données à copier sont au format Parquet.

Le paramètre snowflake_conn_id est l'ID de la connexion Snowflake qui a été configurée 
précédemment pour permettre à Airflow d'interagir avec Snowflake.
