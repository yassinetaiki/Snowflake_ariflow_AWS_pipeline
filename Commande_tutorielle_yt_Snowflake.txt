Lien -> https://quickstarts.snowflake.com/guide/getting_started_with_snowflake/index.html#0

--create table 
create or replace table trips
(tripduration integer,
starttime timestamp,
stoptime timestamp,
start_station_id integer,
start_station_name string,
start_station_latitude float,
start_station_longitude float,
end_station_id integer,
end_station_name string,
end_station_latitude float,
end_station_longitude float,
bikeid integer,
membership_type string,
usertype string,
birth_year integer,
gender integer);

--list-content-bucket
list @citibike_trips;

--create file format

create or replace file format csv type='csv'
  compression = 'auto' field_delimiter = ',' record_delimiter = '\n'
  skip_header = 0 field_optionally_enclosed_by = '\042' trim_space = false
  error_on_column_count_mismatch = false escape = 'none' escape_unenclosed_field = '\134'
  date_format = 'auto' timestamp_format = 'auto' null_if = ('') comment = 'file format for ingesting data for zero to snowflake';


--verify file format is created

show file formats in database citibike;

--load the staged data into the table. This may take up to 30 seconds.
copy into trips from @citibike_trips file_format=csv PATTERN = '.*csv.*' ;

--change warehouse size from small to large (4x) le chargement va etre plus rapide 
--alter warehouse compute_wh set warehouse_size='large';
--show warehouses;



select * from trips limit 10 ;


--Cette requête SQL utilise la fonction date_trunc pour grouper les données par heure à partir de la colonne starttime de la table trips. Elle utilise également la fonction haversine pour calculer la distance moyenne entre les stations de départ et d'arrivée.
--La clause GROUP BY indique la colonne que vous souhaitez utiliser pour grouper les données. Dans ce cas, vous groupez les données par heure en utilisant la fonction date_trunc appliquée à la colonne starttime.La clause ORDER BY indique comment trier les résultats. Dans ce cas, vous triez les résultats par heure.
select date_trunc('hour', starttime) as "date",
count(*) as "num trips",
avg(tripduration)/60 as "avg duration (mins)",
avg(haversine(start_station_latitude, start_station_longitude, end_station_latitude, end_station_longitude)) as "avg distance (km)"
from trips
group by 1 order by 1;

--Snowflake has a result cache that holds the results of every query executed in the past 24 hours. These are available across warehouses, so query results returned to one user are available to any other user on the system who executes the same query, provided the underlying data has not changed. Not only do these repeated queries return extremely fast, but they also use no compute credits.
select date_trunc('hour', starttime) as "date",
count(*) as "num trips",
avg(tripduration)/60 as "avg duration (mins)",
avg(haversine(start_station_latitude, start_station_longitude, end_station_latitude, end_station_longitude)) as "avg distance (km)"
from trips
group by 1 order by 1;

--Next, let's run the following query to see which months are the busiest:
select
monthname(starttime) as "month",
count(*) as "num trips"
from trips
group by 1 order by 2 desc;

--clone of the trips table
create table trips_dev clone trips;

-- create data base for semi structed data
create database weather;

--Execute the following USE commands to set the worksheet context appropriately:

use role accountadmin;
use warehouse compute_wh;
use database weather;
use schema public;

--let's create a table named JSON_WEATHER_DATA to use for loading the JSON data
--Note that Snowflake has a special column data type called VARIANT that allows storing the entire JSON object as a single row and eventually query the object directly
--The VARIANT data type allows Snowflake to ingest semi-structured data without having to predefine the schema.
create table json_weather_data (v variant);

--use the following command to create a stage that points to the bucket where the semi-structured JSON data is stored on AWS S3:
create stage nyc_weather
url = 's3://snowflake-workshop-lab/zero-weather-nyc';

--Now let's take a look at the contents of the nyc_weather
list @nyc_weather

--load data in table json_wetahet_data
copy into json_weather_data from @nyc_weather 
file_format = (type = json strip_outer_array = true);

--Next, let's look at how Snowflake allows us to create a view and also query the JSON data directly using SQL.
create or replace view json_weather_data_view as
select
    v:obsTime::timestamp as observation_time,
    v:station::string as station_id,
    v:name::string as city_name,
    v:country::string as country,
    v:latitude::float as city_lat,
    v:longitude::float as city_lon,
    v:weatherCondition::string as weather_conditions,
    v:coco::int as weather_conditions_code,
    v:temp::float as temp,
    v:prcp::float as rain,
    v:tsun::float as tsun,
    v:wdir::float as wind_dir,
    v:wspd::float as wind_speed,
    v:dwpt::float as dew_point,
    v:rhum::float as relative_humidity,
    v:pres::float as pressure
from
    json_weather_data
where
    station_id = '72502';

--Verify the view with the following query:
--sélectionner les données météorologiques pour le mois de janvier 2018
select * from json_weather_data_view
where date_trunc('month',observation_time) = '2018-01-01'
limit 20;

--We will now join the JSON weather data to our CITIBIKE.PUBLIC.TRIPS data to answer our original question of how weather impacts the number of rides.
select weather_conditions as conditions
,count(*) as num_trips
from citibike.public.trips
left outer join json_weather_data_view
on date_trunc('hour', observation_time) = date_trunc('hour', starttime)
where conditions is not null
group by 1 order by 2 desc;


--Snowflake's powerful Time Travel feature enables accessing historical data, as well as the objects storing the data, at any point within a period of time. The default window is 24 hours and, if you are using Snowflake Enterprise Edition, can be increased up to 90 days

drop table json_weather_data;
undrop table json_weather_data;

--verify table is undropped
select * from json_weather_data limit 10;

--Roll Back a Table
--Run the following command to replace all of the station names in the table with the word "oops":
update trips set start_station_name = 'oops';
--run a query that returns the top 20 stations by number of rides. Notice that the station names result contains only one row:
select
start_station_name as "station",
count(*) as "rides"
from trips
group by 1
order by 2 desc
limit 20;

--In Snowflake, we can simply run a command to find the query ID of the last UPDATE command and store it in a variable named $QUERY_ID
set query_id =
(select query_id from table(information_schema.query_history_by_session (result_limit=>5))
where query_text like 'update%' order by start_time desc limit 1);

--Use Time Travel to recreate the table with the correct station names:
create or replace table trips as
(select * from trips before (statement => $query_id));

--Run the previous query again to verify that the station names have been restored:
select
start_station_name as "station",
count(*) as "rides"
from trips
group by 1
order by 2 desc
limit 20;

--let's create a new role named JUNIOR_DBA and assign it to your Snowflake user
--create the role and assign it to you. Before you run the GRANT ROLE command
drop role junior_dba ;
create role junior_dba ;
grant role junior_dba to user YASSINETAIKI95;

--change your worksheet context to the new JUNIOR_DBA role
--the warehouse is not selected because the newly created role does not have usage privileges on any warehouse.
use role junior_dba;

--change to account admin and grant usage privileges to COMPUTE_WH warehouse acces database 
use role accountadmin;
grant usage on warehouse compute_wh to junior_dba;
grant usage on database citibike to role junior_dba;
grant usage on database weather to role junior_dba;

--verfiy the access of junior_dba
use role junior_dba

--drop all object 
drop share if exists zero_to_snowflake_shared_data;
-- If necessary, replace "zero_to_snowflake-shared_data" with the name you used for the share
drop database if exists citibike;
drop database if exists weather;
drop warehouse if exists analytics_wh;
drop role if exists junior_dba;